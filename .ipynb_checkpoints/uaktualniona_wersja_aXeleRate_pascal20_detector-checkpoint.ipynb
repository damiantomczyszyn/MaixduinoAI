{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hS9yMrWe02WQ"
   },
   "source": [
    "## PASCAL-VOC Detection model Training and Inference\n",
    "\n",
    "In this notebook we will use axelerate, Keras-based framework for AI on the edge, to quickly setup model training and then after training session is completed convert it to .tflite and .kmodel formats.\n",
    "\n",
    "First, let's take care of some administrative details.\n",
    "\n",
    "1) Before we do anything, make sure you have choosen GPU as Runtime type (in Runtime - > Change Runtime type).\n",
    "\n",
    "2) We need to mount Google Drive for saving our model checkpoints and final converted model(s). Press on Mount Google Drive button in Files tab on your left.\n",
    "\n",
    "In the next cell we clone axelerate Github repository and import it.\n",
    "\n",
    "**It is possible to use pip install or python setup.py install, but in that case you will need to restart the enironment.** Since I'm trying to make the process as streamlined as possibile I'm using sys.path.append for import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y3FW3HGfh-Et"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet-mkl==1.6.0 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy==1.23.1 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (1.23.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from mxnet-mkl==1.6.0) (2.31.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from mxnet-mkl==1.6.0) (0.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mxnet-mkl==1.6.0 numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1APOoffMQucw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from keras-preprocessing) (1.23.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from keras-preprocessing) (1.16.0)\n",
      "Installing collected packages: keras-preprocessing\n",
      "Successfully installed keras-preprocessing-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "96h9dakcE7Ln"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit_learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19.5 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from scikit_learn) (1.23.1)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit_learn\n",
      "Successfully installed joblib-1.3.2 scikit_learn-1.4.1.post1 scipy-1.12.0 threadpoolctl-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "y07yAbYbjV2s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping imgaug as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping albumentations as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting imgaug==0.4\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from imgaug==0.4) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from imgaug==0.4) (1.23.1)\n",
      "Requirement already satisfied: scipy in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from imgaug==0.4) (1.12.0)\n",
      "Collecting Pillow\n",
      "  Downloading pillow-10.2.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.8.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-image>=0.14.2\n",
      "  Downloading scikit_image-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting imageio\n",
      "  Downloading imageio-2.34.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Shapely\n",
      "  Downloading shapely-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx>=2.8\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting tifffile>=2022.8.12\n",
      "  Downloading tifffile-2024.2.12-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=21 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from scikit-image>=0.14.2->imgaug==0.4) (23.2)\n",
      "Collecting lazy_loader>=0.3\n",
      "  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.49.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages (from matplotlib->imgaug==0.4) (2.9.0.post0)\n",
      "Collecting numpy>=1.15\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: pyparsing, Pillow, numpy, networkx, lazy_loader, kiwisolver, fonttools, cycler, tifffile, Shapely, opencv-python, imageio, contourpy, scikit-image, matplotlib, imgaug\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.1\n",
      "    Uninstalling numpy-1.23.1:\n",
      "      Successfully uninstalled numpy-1.23.1\n",
      "Successfully installed Pillow-10.2.0 Shapely-2.0.3 contourpy-1.2.0 cycler-0.12.1 fonttools-4.49.0 imageio-2.34.0 imgaug-0.4.0 kiwisolver-1.4.5 lazy_loader-0.3 matplotlib-3.8.3 networkx-3.2.1 numpy-1.26.4 opencv-python-4.9.0.80 pyparsing-3.1.2 scikit-image-0.22.0 tifffile-2024.2.12\n",
      "Cloning into 'aXeleRate'...\n",
      "remote: Enumerating objects: 2367, done.\u001b[K\n",
      "remote: Counting objects: 100% (254/254), done.\u001b[K\n",
      "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
      "remote: Total 2367 (delta 216), reused 190 (delta 184), pack-reused 2113\u001b[K\n",
      "Receiving objects: 100% (2367/2367), 9.65 MiB | 9.04 MiB/s, done.\n",
      "Resolving deltas: 100% (1542/1542), done.\n"
     ]
    }
   ],
   "source": [
    "#we need imgaug 0.4 for image augmentations to work properly, see https://stackoverflow.com/questions/62580797/in-colab-doing-image-data-augmentation-with-imgaug-is-not-working-as-intended\n",
    "#!pip uninstall -y imgaug && pip uninstall -y albumentations && pip install imgaug==0.4\n",
    "#!git clone https://github.com/damiantomczyszyn/aXeleRate.git\n",
    "import sys\n",
    "sys.path.append('/content/aXeleRate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/damian/Desktop/python-venv/MaixduinoAI/aXeleRate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd aXeleRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VAELByZZPfEF"
   },
   "outputs": [],
   "source": [
    "from axelerate import setup_training, setup_inference, setup_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/damian/Desktop/python-venv/MaixduinoAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/damian/Desktop/python-venv/.venv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TBRMPZ83dRL"
   },
   "source": [
    "At this step you typically need to get the dataset. You can use !wget command to download it from somewhere on the Internet or !cp to copy from My Drive as in this example\n",
    "```\n",
    "!cp -r /content/drive/'My Drive'/pascal_20_segmentation.zip .\n",
    "!unzip --qq pascal_20_segmentation.zip\n",
    "```\n",
    "For this notebook we will use PASCAL-VOC 2012 object detection dataset, which you can download here:\n",
    "\n",
    "http://host.robots.ox.ac.uk:8080/pascal/VOC/voc2012/index.html#devkit\n",
    "\n",
    "I split the dataset into training and validation using a simple Python script. Since most of the models trained with aXeleRate are to be run on embedded devices and thus have memory and latency constraints, the validation images are easier than most of the images in training set. The validation images include one(or many) instance of a particular class, no mixed classes in one image.\n",
    "\n",
    "Let's visualize our detection model test dataset. We use img_num=10 to show only first 10 images. Feel free to change the number to None to see all 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_tpsgkGj7d79"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#!gdown https://drive.google.com/uc?id=1xgk7svdjBiEyzyUVoZrCz4PP6dSjVL8S  #pascal-voc dataset\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#!gdown https://drive.google.com/uc?id=1-2jYfTRPX4kSUTL5SUQVxwHKjBclrBTA  #pre-trained model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#!unzip --qq pascal_20_detection.zip\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maxelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualize_detection_dataset \u001b[38;5;28;01mas\u001b[39;00m vdd\n\u001b[0;32m----> 8\u001b[0m \u001b[43mvdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpascal_20_detection/imgs_validation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mann_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpascal_20_detection/anns_validation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_imgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m320\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/python-venv/MaixduinoAI/aXeleRate/axelerate/networks/common_utils/augment.py:185\u001b[0m, in \u001b[0;36mvisualize_detection_dataset\u001b[0;34m(img_folder, ann_folder, num_imgs, img_size, augment)\u001b[0m\n\u001b[1;32m    183\u001b[0m boxes \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mget_boxes(annotation_file)\n\u001b[1;32m    184\u001b[0m img_file \u001b[38;5;241m=\u001b[39m  os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(img_folder, fname)\n\u001b[0;32m--> 185\u001b[0m img, boxes_, labels_ \u001b[38;5;241m=\u001b[39m \u001b[43maug\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(boxes_)):\n\u001b[1;32m    188\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m boxes_[i]\n",
      "File \u001b[0;32m~/Desktop/python-venv/MaixduinoAI/aXeleRate/axelerate/networks/common_utils/augment.py:48\u001b[0m, in \u001b[0;36mImgAugment.imread\u001b[0;34m(self, img_file, boxes, labels)\u001b[0m\n\u001b[1;32m     45\u001b[0m labels_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(labels)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 2. resize and augment image     \u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m image, boxes_, labels_ \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jitter\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, boxes_, labels_\n",
      "File \u001b[0;32m~/Desktop/python-venv/MaixduinoAI/aXeleRate/axelerate/networks/common_utils/augment.py:89\u001b[0m, in \u001b[0;36mprocess_image_detection\u001b[0;34m(image, boxes, labels, desired_w, desired_h, augment)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m     88\u001b[0m     aug_pipe \u001b[38;5;241m=\u001b[39m _create_augment_pipeline()\n\u001b[0;32m---> 89\u001b[0m     image, bbs \u001b[38;5;241m=\u001b[39m \u001b[43maug_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     bbs \u001b[38;5;241m=\u001b[39m bbs\u001b[38;5;241m.\u001b[39mremove_out_of_image()\u001b[38;5;241m.\u001b[39mclip_out_of_image()\n\u001b[1;32m     92\u001b[0m new_boxes, new_labels \u001b[38;5;241m=\u001b[39m _to_array(bbs)\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/imgaug/augmenters/meta.py:2008\u001b[0m, in \u001b[0;36mAugmenter.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Alias for :func:`~imgaug.augmenters.meta.Augmenter.augment`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/imgaug/augmenters/meta.py:1979\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[0;34m(self, return_batch, hooks, **kwargs)\u001b[0m\n\u001b[1;32m   1968\u001b[0m \u001b[38;5;66;03m# augment batch\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m batch \u001b[38;5;241m=\u001b[39m UnnormalizedBatch(\n\u001b[1;32m   1970\u001b[0m     images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m   1971\u001b[0m     heatmaps\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheatmaps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1976\u001b[0m     line_strings\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline_strings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1977\u001b[0m )\n\u001b[0;32m-> 1979\u001b[0m batch_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_batch_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1981\u001b[0m \u001b[38;5;66;03m# return either batch or tuple of augmentables, depending on what\u001b[39;00m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;66;03m# was requested by user\u001b[39;00m\n\u001b[1;32m   1983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_batch:\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/imgaug/augmenters/meta.py:641\u001b[0m, in \u001b[0;36mAugmenter.augment_batch_\u001b[0;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _maybe_deterministic_ctx(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_inaug\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m--> 641\u001b[0m         batch_inaug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_augment_batch_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_inaug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# revert augmentables being set to None for non-activated augmenters\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m set_to_none:\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/imgaug/augmenters/meta.py:3124\u001b[0m, in \u001b[0;36mSequential._augment_batch_\u001b[0;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[1;32m   3121\u001b[0m         order \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mxrange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m   3123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m order:\n\u001b[0;32m-> 3124\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_batch_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/imgaug/augmenters/meta.py:641\u001b[0m, in \u001b[0;36mAugmenter.augment_batch_\u001b[0;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _maybe_deterministic_ctx(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_inaug\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m--> 641\u001b[0m         batch_inaug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_augment_batch_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_inaug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# revert augmentables being set to None for non-activated augmenters\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m set_to_none:\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/imgaug/augmenters/meta.py:3395\u001b[0m, in \u001b[0;36mSomeOf._augment_batch_\u001b[0;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[1;32m   3386\u001b[0m augmenter_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_augmenter_order(random_state)\n\u001b[1;32m   3388\u001b[0m \u001b[38;5;66;03m# create an array of active augmenters per image\u001b[39;00m\n\u001b[1;32m   3389\u001b[0m \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;66;03m#  [[0, 0, 1],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3393\u001b[0m \u001b[38;5;66;03m# would signal, that augmenter 3 is active for the first image,\u001b[39;00m\n\u001b[1;32m   3394\u001b[0m \u001b[38;5;66;03m# augmenter 1 and 3 for the 2nd image and augmenter 1 for the 3rd.\u001b[39;00m\n\u001b[0;32m-> 3395\u001b[0m augmenter_active \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_augmenter_active\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3396\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3398\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m augmenter_index \u001b[38;5;129;01min\u001b[39;00m augmenter_order:\n\u001b[1;32m   3399\u001b[0m     active \u001b[38;5;241m=\u001b[39m augmenter_active[:, augmenter_index]\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/imgaug/augmenters/meta.py:3368\u001b[0m, in \u001b[0;36mSomeOf._get_augmenter_active\u001b[0;34m(self, nb_rows, random_state)\u001b[0m\n\u001b[1;32m   3366\u001b[0m nn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_n(nb_rows, random_state)\n\u001b[1;32m   3367\u001b[0m nn \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nn]\n\u001b[0;32m-> 3368\u001b[0m augmenter_active \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((nb_rows, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)), dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m)\n\u001b[1;32m   3369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_idx, n_true \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nn):\n\u001b[1;32m   3370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_true \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/python-venv/.venv/lib/python3.11/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#!gdown https://drive.google.com/uc?id=1xgk7svdjBiEyzyUVoZrCz4PP6dSjVL8S  #pascal-voc dataset\n",
    "#!gdown https://drive.google.com/uc?id=1-2jYfTRPX4kSUTL5SUQVxwHKjBclrBTA  #pre-trained model\n",
    "#!unzip --qq pascal_20_detection.zip\n",
    "\n",
    "from axelerate.networks.common_utils.augment import visualize_detection_dataset as vdd\n",
    "\n",
    "vdd(img_folder='pascal_20_detection/imgs_validation', ann_folder='pascal_20_detection/anns_validation', num_imgs=10, img_size=320, augment=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azra49tEgAcW"
   },
   "source": [
    "\n",
    "img_folder: Ścieżka do folderu zawierającego obrazy.\n",
    "ann_folder: Ścieżka do folderu zawierającego pliki z etykietami (np. pliki XML lub JSON).\n",
    "num_imgs: Liczba obrazów do wyświetlenia.\n",
    "img_size: Rozmiar obrazów do wyświetlenia.\n",
    "augment: Określa, czy ma być stosowana augmentacja danych przed wyświetleniem obrazów. Augmentacja może obejmować przekształcenia takie jak obrót, przesunięcie, skalowanie itp.\n",
    "Przetwarzanie danych:\n",
    "\n",
    "Funkcja odczytuje obrazy oraz odpowiadające im pliki z etykietami z podanych folderów.\n",
    "Jeśli augment jest ustawione na True, to może przeprowadzić augmentację danych, czyli modyfikacje obrazów w celu zwiększenia różnorodności treningowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1oqdtbr7VLB"
   },
   "source": [
    "Next step is defining a config dictionary. Most lines are self-explanatory.\n",
    "\n",
    "Type is model frontend - Classifier, Detector or Segnet\n",
    "\n",
    "Architecture is model backend (feature extractor)\n",
    "\n",
    "- Full Yolo\n",
    "- Tiny Yolo\n",
    "- MobileNet1_0\n",
    "- MobileNet7_5\n",
    "- MobileNet5_0\n",
    "- MobileNet2_5\n",
    "- SqueezeNet\n",
    "- NASNetMobile\n",
    "- DenseNet121\n",
    "- ResNet50\n",
    "\n",
    "Currently only MobileNet backends available for YOLOv3 detector. I'm working on backend (feature exctractor) overhaul.\n",
    "\n",
    "For more information on anchors, please read here\n",
    "https://github.com/pjreddie/darknet/issues/568\n",
    "\n",
    "Labels are labels present in your dataset.\n",
    "IMPORTANT: Please, list all the labels present in the dataset.\n",
    "\n",
    "object_scale determines how much to penalize wrong prediction of confidence of object predictors\n",
    "\n",
    "no_object_scale determines how much to penalize wrong prediction of confidence of non-object predictors\n",
    "\n",
    "coord_scale determines how much to penalize wrong position and size predictions (x, y, w, h)\n",
    "\n",
    "obj_thresh, nms_threshold set detection confidence threshold and nms thresholds to be used when calcualting precision/recall\n",
    "\n",
    "For converter type you can choose the following:\n",
    "\n",
    "'k210', 'tflite_fullint', 'tflite_dynamic', 'edgetpu', 'openvino', 'onnx'\n",
    "\n",
    "**Since it is an example notebook, we will use pretrained weights and set learning rate to 0.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8Fmpwv_k5hN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jw4q6_MsegD2"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"model\":{\n",
    "            \"type\":                 \"Detector\",\n",
    "            \"architecture\":         \"MobileNet1_0\",\n",
    "            \"input_size\":           [224, 320],\n",
    "            \"anchors\":              [[[0.76120044, 0.57155991], [0.6923348, 0.88535553], [0.47163042, 0.34163313]],\n",
    "                                    [[0.33340788, 0.70065861], [0.18124964, 0.38986752], [0.08497349, 0.1527057 ]]],\n",
    "            \"labels\":               [\"person\", \"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\", \"aeroplane\", \"bicycle\", \"boat\", \"bus\", \"car\", \"motorbike\", \"train\",\"bottle\", \"chair\", \"diningtable\", \"pottedplant\", \"sofa\", \"tvmonitor\"],\n",
    "            \"obj_thresh\" : \t\t    0.7,\n",
    "            \"iou_thresh\" : \t\t    0.5,\n",
    "            \"coord_scale\" : \t\t  1.0,\n",
    "            \"object_scale\" : \t\t  3.0,\n",
    "            \"no_object_scale\" : \t1.0\n",
    "        },\n",
    "        \"weights\" : {\n",
    "            \"full\":   \t\t\t\t  \"/content/yolo_best_recall.h5\",\n",
    "            \"backend\":   \t\t    \"imagenet\"\n",
    "        },\n",
    "        \"train\" : {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"actual_epoch\":         1,\n",
    "            \"train_image_folder\":   \"pascal_20_detection/imgs\",\n",
    "            \"train_annot_folder\":   \"pascal_20_detection/anns\",\n",
    "            \"train_times\":          1,\n",
    "            \"valid_image_folder\":   \"pascal_20_detection/imgs_validation\",\n",
    "            \"valid_annot_folder\":   \"pascal_20_detection/anns_validation\",\n",
    "            \"valid_times\":          1,\n",
    "            \"valid_metric\":         \"recall\",\n",
    "            \"batch_size\":           32,\n",
    "            \"saved_folder\":   \t\tF\"/content/drive/MyDrive/projects/pascal20_yolov3\",\n",
    "            \"first_trainable_layer\": \"\",\n",
    "            \"augmentation\":\t\t\t\t  True,\n",
    "            \"is_only_detect\" : \t\t  False\n",
    "        },\n",
    "        \"converter\" : {\n",
    "            \"type\":   \t\t\t\t[]\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kobC_7gd5mEu"
   },
   "source": [
    "Let's check what GPU we have been assigned in this Colab session, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rESho_T70BWq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0Fc61WrTxh1"
   },
   "source": [
    "Also, let's open Tensorboard, where we will be able to watch model training progress in real time. Training and validation logs also will be saved in project folder.\n",
    "Since there are no logs before we start the training, tensorboard will be empty. Refresh it after first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsGp9JvjTzzp"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "!sleep 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWyKjw-b5_yp"
   },
   "source": [
    "Finally we start the training by passing config dictionary we have defined earlier to setup_training function. The function will start the training with  Reduce Learning Rate on Plateau and save on best mAP callbacks. Every epoch mAP of the model predictions is measured on the validation dataset. If you have specified the converter type in the config, after the training has stopped the script will convert the best model into the format you have specified in config and save it to the project folder.\n",
    "\n",
    "Let's train for one epoch to see how the whole pipeline works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deYD3cwukHsj"
   },
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "#K.clear_session()\n",
    "#model_path = setup_training(config_dict=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypTe3GZI619O"
   },
   "source": [
    "After training it is good to check the actual perfomance of your model by doing inference on your validation dataset and visualizing results. This is exactly what next block does. Our model used pre-trained weights and since all the layers were set as non-trainable, we are just observing the perfomance of the model that was trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jE7pTYmZN7Pi"
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "#from keras import backend as K\n",
    "#K.clear_session()\n",
    "#setup_inference(config, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKsxhdPvzrD8"
   },
   "source": [
    "If you need to convert trained model to other formats, for example for inference with Edge TPU or OpenCV AI Kit, you can do it with following commands. Specify the converter type, backend and folder with calbiration images(normally your validation image folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awR7r4ILzrmb"
   },
   "outputs": [],
   "source": [
    "#from axelerate.networks.common_utils.convert import Converter\n",
    "#converter = Converter('tflite_dynamic', 'MobileNet1_0', 'pascal_20_detection/imgs_validation')\n",
    "#converter.convert_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPvYzcRhfs2u"
   },
   "source": [
    "To train the model from scratch use the following config and then run the cells with training and (optinally) inference functions again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uruWpeGRf6Qi"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"model\":{\n",
    "            \"type\":                 \"Detector\",\n",
    "            \"architecture\":         \"MobileNet1_0\",\n",
    "            \"input_size\":           [224, 320],\n",
    "            \"anchors\":              [[[0.76120044, 0.57155991], [0.6923348, 0.88535553], [0.47163042, 0.34163313]],\n",
    "                                    [[0.33340788, 0.70065861], [0.18124964, 0.38986752], [0.08497349, 0.1527057 ]]],\n",
    "            \"labels\":               [\"person\", \"bird\", \"cat\", \"cow\", \"dog\", \"horse\", \"sheep\", \"aeroplane\", \"bicycle\", \"boat\", \"bus\", \"car\", \"motorbike\", \"train\",\"bottle\", \"chair\", \"diningtable\", \"pottedplant\", \"sofa\", \"tvmonitor\"],\n",
    "            \"obj_thresh\" : \t\t    0.7,\n",
    "            \"iou_thresh\" : \t\t    0.5,\n",
    "            \"coord_scale\" : \t\t  1.0,\n",
    "            \"object_scale\" : \t\t  3.0,\n",
    "            \"no_object_scale\" : \t1.0\n",
    "        },\n",
    "        \"weights\" : {\n",
    "            \"full\":   \t\t\t\t  \"\",\n",
    "            \"backend\":   \t\t    \"imagenet\"\n",
    "        },\n",
    "        \"train\" : {\n",
    "            \"actual_epoch\":         50,\n",
    "            \"train_image_folder\":   \"pascal_20_detection/imgs\",\n",
    "            \"train_annot_folder\":   \"pascal_20_detection/anns\",\n",
    "            \"train_times\":          1,\n",
    "            \"valid_image_folder\":   \"pascal_20_detection/imgs_validation\",\n",
    "            \"valid_annot_folder\":   \"pascal_20_detection/anns_validation\",\n",
    "            \"valid_times\":          1,\n",
    "            \"valid_metric\":         \"recall\",\n",
    "            \"batch_size\":           32,\n",
    "            \"learning_rate\":        1e-3,\n",
    "            \"saved_folder\":   \t\tF\"/content/drive/MyDrive/projects/pascal20_yolov3\",\n",
    "            \"first_trainable_layer\": \"\",\n",
    "            \"augmentation\":\t\t\t\t  True,\n",
    "            \"is_only_detect\" : \t\t  False\n",
    "        },\n",
    "        \"converter\" : {\n",
    "            \"type\":   \t\t\t\t[]\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1frVrWMcf-k7"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "model_path = setup_training(config_dict=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ipv1AGzRgAMA"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "setup_inference(config, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YuVe2VD11cd"
   },
   "source": [
    "Good luck and happy training! Have a look at these articles, that would allow you to get the most of Google Colab or connect to local runtime if there are no GPUs available;\n",
    "\n",
    "https://medium.com/@oribarel/getting-the-most-out-of-your-google-colab-2b0585f82403\n",
    "\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
